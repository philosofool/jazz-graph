{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch_geometric.nn import GraphConv, SAGEConv, to_hetero, HeteroConv\n",
    "from torch_geometric import transforms as T\n",
    "from torch_geometric import seed_everything\n",
    "\n",
    "from jazz_graph.data.utils import inspect_degrees\n",
    "from jazz_graph.pyg_data.pyg_data import CreateTensors\n",
    "from jazz_graph.model import JazzModel, LinkPredictionModel, NodeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = '/workspace/local_data/graph_parquet_proto'\n",
    "create = CreateTensors(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: report on the data a little more concreately.\n",
    "# E.g., who are the hub nodes? How many nodes have > 50 edges.\n",
    "# how many nodes have < 6 edges? All these, by type.\n",
    "# Get really fancy and visualize a sub-graph.\n",
    "\n",
    "def frequency_of_n_labels(data: HeteroData):\n",
    "    \"\"\"Return frequency of number of labels in the data, i.e., what percentage have 1 label, 0 labels, etc.\"\"\"\n",
    "    count_by_row = data['performance'].y.sum(dim=1)\n",
    "    n_samples = data['performance'].y.shape[0]\n",
    "    counter = Counter((int(x) for x in (count_by_row)))\n",
    "    for i in range(len(counter)):\n",
    "        count = counter[i]\n",
    "        freq = count / n_samples\n",
    "        print(f\"Num samples with {i} labels: {freq:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "\n",
    "def index_tensor(tensor):\n",
    "    \"\"\"Return 0, 1, 2... for each value in tensor. (An index.)\n",
    "\n",
    "    When sampling graph nodes, we want a direct lookup of the node\n",
    "    ids.\n",
    "    \"\"\"\n",
    "    return torch.arange(0, tensor.size(0), dtype=torch.int64).reshape(-1, 1)\n",
    "\n",
    "# This is a little clunky. The nodes are not expected to provide\n",
    "# substantial feature information--the information is the graph.\n",
    "data['performance'].x = index_tensor(create.performances())\n",
    "data['song'].x = index_tensor(create.songs())\n",
    "data['artist'].x = index_tensor(create.artists())\n",
    "\n",
    "data['artist', 'performs', 'performance'].edge_index = create.artist_performance_edges()\n",
    "data['performance', 'performing', 'song'].edge_index = create.performance_song_edges()\n",
    "data['artist', 'composed', 'song'].edge_index = create.artist_song_edges()\n",
    "\n",
    "data['performance'].y = create.labels()\n",
    "data['performance'].train_mask = create.train_mask()\n",
    "data['performance'].dev_mask = create.dev_mask()\n",
    "data['performance'].test_mask = create.test_mask()\n",
    "\n",
    "# data['artist', 'performs', 'performance'].edge_attr = <instrument>\n",
    "data = ToUndirected()(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create.label_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)\n",
    "print(\n",
    "    f\"The graph contains {'' if data.has_isolated_nodes() else 'no '}isolated nodes and\",\n",
    "    f\"is {'directed' if data.is_directed() else 'undirected'}.\"\n",
    ")\n",
    "frequency_of_n_labels(data)\n",
    "for style, count in (zip(create.label_names(), data['performance'].y.sum(dim=0))):\n",
    "    print(f\"  {style}: {int(count) / create._labels.shape[0]:.1%}\")\n",
    "    # Easy Listening is probably a mislabel by modern standards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>performs</th>\n",
       "      <th>performing</th>\n",
       "      <th>composed</th>\n",
       "      <th>rev_performs</th>\n",
       "      <th>rev_performing</th>\n",
       "      <th>rev_composed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2583.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>2583.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.656601</td>\n",
       "      <td>0.402535</td>\n",
       "      <td>1.696477</td>\n",
       "      <td>3.003123</td>\n",
       "      <td>2.058243</td>\n",
       "      <td>2.058243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22.430807</td>\n",
       "      <td>0.634782</td>\n",
       "      <td>7.352685</td>\n",
       "      <td>4.485468</td>\n",
       "      <td>2.449948</td>\n",
       "      <td>2.449948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>326.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          performs    performing     composed  rev_performs  rev_performing  \\\n",
       "count  2583.000000  10886.000000  2583.000000  10886.000000     2129.000000   \n",
       "mean     12.656601      0.402535     1.696477      3.003123        2.058243   \n",
       "std      22.430807      0.634782     7.352685      4.485468        2.449948   \n",
       "min       0.000000      0.000000     0.000000      0.000000        1.000000   \n",
       "25%       0.000000      0.000000     0.000000      0.000000        1.000000   \n",
       "50%       6.000000      0.000000     0.000000      1.000000        1.000000   \n",
       "75%      13.000000      1.000000     1.000000      5.000000        2.000000   \n",
       "max     326.000000      8.000000   153.000000     51.000000       30.000000   \n",
       "\n",
       "       rev_composed  \n",
       "count   2129.000000  \n",
       "mean       2.058243  \n",
       "std        2.449948  \n",
       "min        1.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        2.000000  \n",
       "max       30.000000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect_degrees(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NodeClassifier(\n",
    "    JazzModel(\n",
    "        data['performance'].num_nodes,\n",
    "        data['artist'].num_nodes,\n",
    "        data['song'].num_nodes,\n",
    "        hidden_dim=128,\n",
    "        embed_dim=64,\n",
    "        metadata=data.metadata()\n",
    "    ),\n",
    "    hidden_dim=128,\n",
    "    num_classes=len(create.label_names())\n",
    ")\n",
    "\n",
    "data['performance'].num_nodes\n",
    "data['artist'].num_nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "def train_indicies(mask):\n",
    "    num_nodes = mask.shape[0]\n",
    "    all_node_indicies = torch.arange(num_nodes)\n",
    "    return all_node_indicies[mask]\n",
    "\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    [15, 15, 15],\n",
    "    batch_size=128,\n",
    "    input_nodes=('performance', train_indicies(data['performance'].train_mask)),\n",
    ")\n",
    "dev_loader = NeighborLoader(\n",
    "    data,\n",
    "    [15, 15, 15],\n",
    "    batch_size=128,\n",
    "    input_nodes=('performance', train_indicies(data['performance'].dev_mask)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set up some kind of proper training logs.\n",
    "# You know yourself and you WILL end up running dozens\n",
    "# of runs with small configuration tweaks. Without logs\n",
    "# you have no way to recover what you did.\n",
    "\n",
    "\n",
    "# I pulled this from some other course work that I did.\n",
    "class ConfusionMatrix:\n",
    "    \"\"\"Confusion matrix calculator which can accumulate predictions and compute them.\n",
    "\n",
    "    Currently supports binary labels in a multi-label configuration and generates\n",
    "    n label one-versus-rest confusion matrices.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # set TP, TN, FP, FN\n",
    "        self.reset()\n",
    "\n",
    "    def update(self, pred: torch.Tensor, labels: torch.Tensor):\n",
    "        \"\"\"Update the confusion matrix, for example, during each batch of training.\"\"\"\n",
    "        predicted = (pred > .5)\n",
    "        self.true_positives = ((predicted == labels) & (labels == 1)).sum(dim=0) + self.true_positives\n",
    "        self.false_positives = ((predicted != labels) & (labels == 1)).sum(dim=0) + self.false_positives\n",
    "        self.true_negatives = ((predicted == labels) & (labels == 0)).sum(dim=0) + self.true_negatives\n",
    "        self.false_negatives = ((predicted != labels) & (labels == 0)).sum(dim=0) + self.false_negatives\n",
    "        # print(\"TP\", self.true_positives)\n",
    "\n",
    "    def compute(self) -> np.ndarray:\n",
    "        \"\"\"Compute the confusion matrix and return it.\n",
    "\n",
    "        The result is a n-labels one-versus-rest binary confusion matrices\n",
    "        with shape (2, 2, n_labels.)\n",
    "        \"\"\"\n",
    "        out = np.zeros((2, 2, self.true_negatives.shape[-1]))\n",
    "        out[0, 0] = self.true_negatives.cpu().numpy()\n",
    "        out[0, 1] = self.false_negatives.cpu().numpy()\n",
    "        out[1, 0] = self.false_positives.cpu().numpy()\n",
    "        out[1, 1] = self.true_positives.cpu().numpy()\n",
    "        return out\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the matrix to zeros, for example, after each complete epoch.\"\"\"\n",
    "        self.true_positives = torch.tensor(0)\n",
    "        self.false_positives = torch.tensor(0)\n",
    "        self.true_negatives = torch.tensor(0)\n",
    "        self.false_negatives = torch.tensor(0)\n",
    "\n",
    "\n",
    "def per_label_accuracy(confusion: ConfusionMatrix):\n",
    "    confusion_ = confusion.compute()\n",
    "    true_positives, false_positives, true_negatives, false_negatives = confusion_[1, 1], confusion_[1, 0], confusion_[0, 0], confusion_[0, 1]\n",
    "    numer = true_positives + true_negatives\n",
    "    return numer / (numer + false_positives + false_negatives)\n",
    "\n",
    "def per_label_precision(confusion: ConfusionMatrix):\n",
    "    confusion_ = confusion.compute()\n",
    "    true_positives, false_positives, true_negatives, false_negatives = confusion_[1, 1], confusion_[1, 0], confusion_[0, 0], confusion_[0, 1]\n",
    "\n",
    "    denom = (true_positives + false_positives)\n",
    "    macro_raw = np.nan_to_num(true_positives / denom)\n",
    "    macro = np.sum(macro_raw) / denom.shape[0]\n",
    "    micro = true_positives.sum() / (true_positives.sum() + false_positives.sum())\n",
    "\n",
    "    return micro, macro\n",
    "\n",
    "    # class_counts = true_positives + false_positives + true_negatives + false_negatives\n",
    "    # weighted_raw = np.nan_to_num((true_positives * class_counts) / (denom * class_counts))\n",
    "    # weighted = np.sum(weighted_raw)\n",
    "    # print(micro, macro, weighted)\n",
    "    # return micro, macro, weighted\n",
    "\n",
    "\n",
    "def per_label_recall(confusion: ConfusionMatrix):\n",
    "    confusion_ = confusion.compute()\n",
    "    true_positives, false_positives, true_negatives, false_negatives = confusion_[1, 1], confusion_[1, 0], confusion_[0, 0], confusion_[0, 1]\n",
    "\n",
    "    denom = (true_positives + false_negatives)\n",
    "    macro_raw = np.nan_to_num(true_positives / denom)\n",
    "    macro = np.sum(macro_raw) / denom.shape[0]\n",
    "    micro = true_positives.sum() / (true_positives.sum() + false_negatives.sum())\n",
    "\n",
    "    return micro, macro\n",
    "\n",
    "def format_float_arr(arr):\n",
    "    strings = [f'{x:.3f}' for x in arr]\n",
    "    return ', '.join(strings)\n",
    "\n",
    "def batch_report(confusion: ConfusionMatrix, batch: int):\n",
    "    print(f\"Finished batch {batch}.\")\n",
    "    print(format_float_arr(per_label_accuracy(confusion)))\n",
    "    confusion.reset()\n",
    "\n",
    "def epoch_report(confusion: ConfusionMatrix, epoch: int):\n",
    "    print(f\"Finished epoch {epoch}.\")\n",
    "    print(\"  Accuracies: \", format_float_arr(per_label_accuracy(confusion)))\n",
    "    print(\"  Recalls: \", format_float_arr(per_label_recall(confusion)))\n",
    "    print(\"  Precisions: \", format_float_arr(per_label_precision(confusion)))\n",
    "    confusion.reset()\n",
    "\n",
    "\n",
    "class WeightedF1Score:\n",
    "    def __init__(self):\n",
    "        self.confusion = ConfusionMatrix()\n",
    "        self.name = 'weighted_f1_score'\n",
    "\n",
    "    def update(self, loss, pred, labels):\n",
    "        self.confusion.update(loss, pred, labels)\n",
    "\n",
    "    def reset(self):\n",
    "        self.confusion.reset()\n",
    "\n",
    "    def compute(self) -> np.ndarray:\n",
    "        confusion = self.confusion.compute()\n",
    "        true_positives, false_positives, true_negatives, false_negatives = confusion[1, 1], confusion[1, 0], confusion[0, 0], confusion[0, 1]\n",
    "        f1 = (2 * true_positives) / (2 * true_positives + false_positives + false_negatives)\n",
    "        weight = true_positives + false_negatives\n",
    "        weighted_f1 = (f1 * weight) / (weight.sum())\n",
    "        return np.sum(weighted_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_confusion():\n",
    "    confusion = ConfusionMatrix()\n",
    "    predicitions = torch.tensor([\n",
    "        [.1, .8, .2], [.2, .7, .1]\n",
    "    ])\n",
    "    labels = torch.tensor([\n",
    "        [0, 1, 1], [0, 1, 0]\n",
    "    ])\n",
    "    confusion.update(predicitions, labels)\n",
    "    np.testing.assert_array_equal(confusion.true_negatives, np.array([2, 0, 1]))\n",
    "    np.testing.assert_array_equal(confusion.true_positives, np.array([0, 2, 0]))\n",
    "    np.testing.assert_array_equal(confusion.false_negatives, np.array([0, 0, 0]))\n",
    "    np.testing.assert_array_equal(confusion.false_positives, np.array([0, 0, 1]))\n",
    "\n",
    "    matrix = confusion.compute()\n",
    "    np.testing.assert_array_equal(matrix[0, 0], confusion.true_negatives)\n",
    "    np.testing.assert_array_equal(matrix[0, 1], confusion.false_negatives)\n",
    "    np.testing.assert_array_equal(matrix[1, 0], confusion.false_positives)\n",
    "    np.testing.assert_array_equal(matrix[1, 1], confusion.true_positives)\n",
    "\n",
    "test_confusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def train(model: NodeClassifier, loader: NeighborLoader, dev_loader: NeighborLoader, epochs: int = 1):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "\n",
    "    batch_confusion = ConfusionMatrix()\n",
    "    epoch_confusion = ConfusionMatrix()\n",
    "    losses = defaultdict(list)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    default_batch_size = loader.batch_size\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_loss = 0\n",
    "        n_samples = 0\n",
    "        for i, batch in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_size = batch['performance'].batch_size\n",
    "            y_hat = model(batch.x_dict, batch.edge_index_dict)[:batch_size]\n",
    "            y = batch['performance'].y[:batch_size]\n",
    "            loss = criterion(y_hat, y.to(torch.float))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss += loss.item() * batch_size\n",
    "            n_samples += batch_size\n",
    "            batch_confusion.update(y_hat, y)\n",
    "            epoch_confusion.update(y_hat, y)\n",
    "\n",
    "        losses['train'].append(batch_loss / n_samples)\n",
    "        epoch_report(epoch_confusion, epoch)\n",
    "\n",
    "        for i, batch in enumerate(dev_loader):\n",
    "            model.eval()\n",
    "            batch_loss = 0\n",
    "            n_samples = 0\n",
    "            batch_size = batch['performance'].batch_size\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(batch.x_dict, batch.edge_index_dict)[:batch_size]\n",
    "                y = batch['performance'].y[:batch_size]\n",
    "                loss = criterion(y_hat, y.to(torch.float))\n",
    "\n",
    "            batch_loss += loss.item() * batch_size\n",
    "            n_samples += batch_size\n",
    "            epoch_confusion.update(y_hat, y)\n",
    "        losses['val_loss'].append(batch_loss / n_samples)\n",
    "        print(\"Dev set results.\")\n",
    "        epoch_report(epoch_confusion, epoch)\n",
    "    return losses\n",
    "\n",
    "losses = train(model, train_loader, dev_loader, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "for key, loss in losses.items():\n",
    "    ax.plot(loss, label = key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a random sample of predictions:\n",
    "confusion = ConfusionMatrix()\n",
    "all_preds = []\n",
    "for batch in dev_loader:\n",
    "    with torch.no_grad():\n",
    "        y_hat = F.sigmoid(model(batch.x_dict, batch.edge_index_dict))\n",
    "        y = batch['performance'].y\n",
    "    confusion.update(y_hat, y)\n",
    "    all_preds.append(y_hat)\n",
    "\n",
    "probs = np.concatenate(all_preds)\n",
    "selections = probs > .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion.compute()[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks: are we making positive predictions? is the number of positive per sample close to 1?\n",
    "print(f\"A total of {(selections.sum(axis=1) > 0).sum()} of {selections.shape[0]} samples received some classification.\")\n",
    "print(pd.Series(selections.sum(axis=1)).value_counts())\n",
    "selections.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Prediction\n",
    "\n",
    "Quick shot at writing an edge prediction model. \n",
    "Conceptually, a recommender system based on this prediction \"who worked with whom, on what?\"\n",
    "Thus, it's a lot like asking Theolious Monk \"What would you recommend?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "performs_edge_count = data[('artist', 'performs', 'performance')].num_edges\n",
    "\n",
    "split_graph = T.RandomLinkSplit(\n",
    "    num_val=int(performs_edge_count * .1),\n",
    "    num_test=int(performs_edge_count * .1),\n",
    "    disjoint_train_ratio=.3,\n",
    "    neg_sampling_ratio=2.0,\n",
    "    add_negative_train_samples=False,\n",
    "    edge_types=('artist', 'performs', 'performance'),\n",
    "    rev_edge_types=('performance', 'rev_performs', 'artist')\n",
    ")\n",
    "train_data, dev_data, test_data = split_graph(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "def edge_training_data_factory(data: HeteroData) -> LinkNeighborLoader:\n",
    "    edge_loader = LinkNeighborLoader(\n",
    "        data=data,\n",
    "        num_neighbors=[15, 15],\n",
    "        neg_sampling_ratio=2.0,\n",
    "        edge_label_index=(('artist', 'performs', 'performance'), train_data['performs'].edge_label_index),\n",
    "        edge_label=None,\n",
    "        batch_size=128,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return edge_loader\n",
    "\n",
    "edge_loader_train = edge_training_data_factory(train_data)\n",
    "edge_loader_dev = edge_training_data_factory(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinkPredictionModel(JazzModel(\n",
    "    data['performance'].num_nodes,\n",
    "    data['artist'].num_nodes,\n",
    "    data['song'].num_nodes,\n",
    "    hidden_dim=128,\n",
    "    embed_dim=64,\n",
    "    metadata=data.metadata()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(edge_loader_train))\n",
    "batch['performs'].edge_label\n",
    "batch['performs'].edge_label\n",
    "batch['performs'].edge_label_index.shape\n",
    "model(batch.x_dict, batch.edge_index_dict, batch['performs'].edge_label_index).shape\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNTrainingLogic:\n",
    "    \"\"\"Define training step and eval steps.\"\"\"\n",
    "    def __init__(self, model, optimizer, criterion):\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def _extract_model_args(self, batch):\n",
    "        return batch.x_dict, batch.edge_index_dict, batch['performs'].edge_label_index\n",
    "\n",
    "    def train_step(self, engine, batch: HeteroData) -> dict:\n",
    "        \"\"\"Complete one step of gradient descent.\"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        batch.to(self.device)\n",
    "\n",
    "        y_pred = self.model(*self._extract_model_args(batch))\n",
    "        y_true = batch['performs'].edge_label\n",
    "        loss = self.criterion(y_pred, y_true)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return {'loss': loss.item(), 'y_pred': y_pred.detach(), 'y_true': y_true.detach()}\n",
    "\n",
    "    def eval_step(self, engine, batch: HeteroData) -> dict:\n",
    "        \"\"\"Complete one pass over a batch of data with no-grad and return results.\"\"\"\n",
    "        self.model.eval()\n",
    "        batch.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(*self._extract_model_args(batch))\n",
    "            y_true = batch['performs'].edge_label\n",
    "        return {'y_pred': y_pred, 'y_true': y_true}\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "trainer_logic = GNNTrainingLogic(model, optimizer, criterion)\n",
    "trainer_logic.train_step(None, batch) is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Recall, Precision, Accuracy, Loss\n",
    "\n",
    "def log_training_results(trainer, evaluator, loader, step_name):\n",
    "    evaluator.run(loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"{step_name} - Epoch[{trainer.state.epoch:03}]\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  Avg. {metric}: {value:.3f}\")\n",
    "\n",
    "def binary_output_transform(output: dict[str, torch.Tensor]) -> tuple:\n",
    "    \"\"\"Return y_true and y_pred as binary classifications.\"\"\"\n",
    "    y_pred = (output[\"y_pred\"] > 0).long()\n",
    "    y_true = output[\"y_true\"]\n",
    "    return y_pred, y_true\n",
    "\n",
    "accuracy = Accuracy(output_transform=binary_output_transform)\n",
    "\n",
    "trainer = Engine(trainer_logic.train_step)\n",
    "train_evaluator = Engine(trainer_logic.eval_step)\n",
    "dev_evaluator = Engine(trainer_logic.eval_step)\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': Accuracy(output_transform=binary_output_transform),\n",
    "    'recall': Recall(output_transform=binary_output_transform),\n",
    "    'precision': Precision(output_transform=binary_output_transform),\n",
    "    'loss': Loss(criterion, output_transform=lambda out: (out['y_pred'], out['y_true']))\n",
    "}\n",
    "\n",
    "for name, metric in metrics.items():\n",
    "    metric.attach(train_evaluator, name)\n",
    "    metric.attach(dev_evaluator, name)\n",
    "\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, log_training_results, train_evaluator, edge_loader_train, \"Training\")\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, log_training_results, dev_evaluator, edge_loader_dev, \"Validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(edge_loader_train, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
